{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # This part of code is the DQN brain, which is the brain of the agent.\n",
    "# All decisions are made in here.\n",
    "\n",
    "# Two networks are created in class DeepQNetwork, namely evaluate_net and target_net, which respectively output Q estimation and Q reality, use the difference between these two values to construct the loss function, and update the evaluate_net parameter. When selecting an action, use the Q-learning algorithm.\n",
    "\n",
    "# In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.n_actions = kwargs.get('n_actions')\n",
    "        self.n_features = kwargs.get('n_features')\n",
    "        self.lr = kwargs.get('learning_rate', 0.00025)\n",
    "        self.gamma = kwargs.get('reward_decay', 0.99) #Reward attenuation factor\n",
    "        self.epsilon_max = kwargs.get('e_greedy', 0.9)\n",
    "        self.replace_target_iter = kwargs.get('replace_target_iter', 300) #Number of steps to update Q realistic network parameters\n",
    "        self.memory_size = kwargs.get('memory_size', 500) #Number of stored memories\n",
    "        self.batch_size = kwargs.get('batch_size', 64) #The number of samples taken from the memory each time, stochastic gradient descent SGD will be used\n",
    "        self.epsilon_increment = kwargs.get('e_greedy_increment') #Increase epsilon so that there is a greater probability of getting the best value\n",
    "        self.epsilon = kwargs.get('epsilon', 0.5 if self.epsilon_increment is not None else self.epsilon_max) #If e_greedy_increment has no value, self.epsilon is set to self.epsilon_max=0.9\n",
    "        self.output_graph = kwargs.get('output_graph')\n",
    "        self.log = kwargs.get('log', print) # logging function\n",
    "        self.statusPeriod = kwargs.get('statusPeriod', 1) # period at which to report status\n",
    "        self.flag = 0\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0#Record the number of steps learned，Self.epsilon continues to improve based on this number of steps\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, self.n_features * 2 + 2))\n",
    "        tf.reset_default_graph()\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        if self.output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "        init= tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        self.cost_hist = [] #a cost table that records the error of each step\n",
    "        #saver = tf.train.Saver(max_to_keep = 1)\n",
    "       # save_path = saver.save(self.sess, \"my_net/my_test_model.ckpt\")\n",
    "        #print(\"Save to path:\", save_path)\n",
    "\n",
    "    # sess = tf.Session()\n",
    "    # target_net = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "    # l1_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net/l1')\n",
    "    # l2_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net/l2')\n",
    "    # saver1 = tf.train.Saver(target_net, max_to_keep=0)\n",
    "    # saver2 = tf.train.Saver(l1_params, max_to_keep=0)\n",
    "    # saver3 = tf.train.Saver(l2_params, max_to_keep=0)\n",
    "    # save_path1 = saver1.save(sess, \"./my_net/target_net.ckpt\")\n",
    "    # save_path2 = saver2.save(sess, \"./my_net/l1_params.ckpt\")\n",
    "    # save_path3 = saver3.save(sess, \"./my_net/l2_params.ckpt\")\n",
    "\n",
    "    def _build_net(self):\n",
    "        # Building a network\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.env_state = tf.placeholder(tf.float32, [None, self.n_features], name='action1')\n",
    "        #self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # Enter the current state as the input to the NN\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # Input Q reality for backward error calculation\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) are the collections to store variable\n",
    "            c_names, n_l1, w_initializer, b_initializer = ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10,                                                           tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "            # tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.tanh(tf.matmul(self.env_state, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))#Constructing a loss-function based on Q estimates and Q reality\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)#Training\n",
    "\n",
    "        # ------------------ build target_net ------------------The target value network predicts that the Qtarget parameter is the previous one, and finally the output q_next\n",
    "        self.env_state_= tf.placeholder(tf.float32, [None, self.n_features], name='env_state_')    # Input s_ indicates the next state, and q_target is calculated with the next state\n",
    "        \n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) are the collections to store variables，Put q realistic parameters into this collection\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):#This is the same as the previous network structure, except that the stored parameters are different, because the target network will not be trained later, but the estimated network is updated separately.\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.tanh(tf.matmul(self.env_state_, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "\n",
    "\n",
    "    def store_transition(self, env_state, a, r, env_state_):#Storage memory\n",
    "        if not hasattr(self, 'memory_counter'):#hasattr(object, name)Determine whether there is a name attribute or a name method in an object, return the BOOL value, and the index item does not exist at the beginning.\n",
    "            self.memory_counter = 0#Determines that the self object has a name attribute that returns True, otherwise returns False. That is, without this index value memory_counter, let self.memory_counter=0\n",
    "\n",
    "        transition = np.hstack((env_state, [a, r], env_state_))#The numpy.hstack(tup) parameter tup can be a tuple, list, or numpy array, and the result is an array of numpys stacked in order (one by column).\n",
    "\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition #Replace the index line in memory with an array of transition lines.\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, env_state):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "\n",
    "        env_state = env_state[np.newaxis, :]\n",
    "\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:#np.random.uniform generates a uniformly distributed random number, the default is 0-1, the maximum probability of selecting the action_value maximum action\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.env_state: env_state})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _replace_target_params(self,flag):\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "\n",
    "\n",
    "        self.sess.run([tf.assign(t, e) for t, e in zip(t_params, e_params)])\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        #Check if the target_net parameter is replaced in advance, and self.learn_step_counter records the number of steps.\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self._replace_target_params(self.learn_step_counter)\n",
    "            print(self.learn_step_counter)\n",
    "            self.log('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:#If the number of steps that need to be remembered exceeds the memory capacity\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)#Generate a random sample from the given one-dimensional array self.memory_size, size is Output shape.\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)#If the number of steps does not exceed the total memory capacity, then at most 32 index values are selected in the self.memory_counter memory values.\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        #Running these two neural networks\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.env_state_: batch_memory[:, -self.n_features:],  # Fixed params, q_next is entered by the target value network using the value of the n_features column (observation_) in the memory bank.\n",
    "                self.env_state: batch_memory[:, :self.n_features],  # Newest params, q_eval is entered by the predicted value network using the value of the positive n_features columns in the memory.\n",
    "            })\n",
    "       # print(self.learn_step_counter)\n",
    "        #Save model parameters\n",
    "        if self.learn_step_counter % 1000 == 0:\n",
    "            t_params = tf.get_collection('target_net_params')\n",
    "            saver1 = tf.train.Saver(t_params,max_to_keep=1)  # , max_to_keep=0)\n",
    "            saver1.save(self.sess, \"./my_net/target_net.ckpt\",global_step=self.learn_step_counter,write_meta_graph=False)\n",
    "\n",
    "        # change q_target w.r.t q_eval's action\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)#Returns a list of index values of length self.batch_size aray([0,1,2,...,31])\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)#Returns a list of actions of length 32, from the second column of the tag in the memory batch_memory, self.n_features=2\n",
    "        reward = batch_memory[:, self.n_features + 1]#Returns a list of 32 rewards, extracting the reward from the memory\n",
    "       \n",
    "      \n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "        #Returns a 32*4 np.array form, q_next is output by the target network (sample number *4), and 32 inputs are taken from the memory to the network.\n",
    "\n",
    "        # train eval network\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.env_state: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_hist.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        \n",
    "    def plot_cost(self):\n",
    "        fig = plt.figure()\n",
    "        plt.plot(self.cost_hist)\n",
    "\n",
    "        print(\"cost_hist's Length is {}\".format(len(self.cost_hist)))\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Training steps')\n",
    "        plt.show()\n",
    "\n",
    "        # Return fig handle for plot modification outside of this function.\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
