{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model.ipynb\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from Utils import NotebookFinder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "train_num_examples = 893231#总数：1116540\n",
    "Iteration =13957\n",
    "BATCH_SIZE = model.BATCH_SIZE\n",
    "LEARNING_RATE_BASE = 0.005 # 初始学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率的衰减率\n",
    "Sequence_index_train= 0\n",
    "Sequence_index_test= 0\n",
    "REGULARIZER = 0.0001\n",
    "MOVING_AVERAGE_DECAY = 0.99 # 滑动平均的衰减率\n",
    "width = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = []\n",
    "labels_train = []\n",
    "Sequence_index_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_train(Iteration,batch_size):  \n",
    "    global features_train\n",
    "    global labels_train\n",
    "    global Sequence_index_train\n",
    "    start_train = Sequence_index_train \n",
    "    Length_train = len(labels_train)\n",
    "  #  print(Length_train)\n",
    "    if Iteration == 0:\n",
    "        Sequence_index_train = 0\n",
    "        start_train = 0\n",
    "        index_train = [ i for i in range(0,Length_train)]  \n",
    "        np.random.shuffle(index_train)\n",
    "        features_train = features_train[index_train]\n",
    "        labels_train = labels_train[index_train]\n",
    "#     if start_train + batch_size > Length_train: \n",
    "#         rest_num_examples = Length_train - start_train\n",
    "#         features_rest_part = features_train[start_train:Length_train]\n",
    "#         labels_rest_part = labels_train[start_train:Length_train]\n",
    "#         index_train = [ i for i in range(0,Length_train) ]  \n",
    "#         np.random.shuffle(index_train) \n",
    "\n",
    "#         features_train= features_train[index_train]\n",
    "#         labels_train = labels_train[index_train]\n",
    "#         start_strain = 0\n",
    "#         Sequence_index_train = batch_size - rest_num_examples\n",
    "#         end_train = Sequence_index_train\n",
    "#         features_new_part = features_train[start_train:end_train]\n",
    "#         labels_new_part = labels_train[start_train:end_train]\n",
    "\n",
    "#         return np.concatenate((features_rest_part, features_new_part), axis=0), np.concatenate(\n",
    "#                 (labels_rest_part, labels_new_part), axis=0)\n",
    "#     else:\n",
    "    Sequence_index_train += batch_size\n",
    "    end_train = Sequence_index_train\n",
    "#     print(start_train,end_train)\n",
    "#     print(labels_train[start_train:end_train])\n",
    "    return features_train[start_train:end_train],labels_train[start_train:end_train]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(features_train_,labels_train_):\n",
    "    tf.reset_default_graph()\n",
    "    features = tf.placeholder(tf.float32, [None,81])\n",
    "    labels = tf.placeholder(tf.int32, [None])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0, trainable=False)# 声明一个全局计数器，并输出化为 0\n",
    "\n",
    "    t_logits = model.inference(features,REGULARIZER,keep_prob)\n",
    "    global features_train\n",
    "    global labels_train\n",
    "    features_train = features_train_\n",
    "    labels_train = labels_train_\n",
    "    \n",
    "    # First, do a softmax on the output y of the last layer of the network, usually to find the probability that the output belongs to a certain class, which is actually a vector of size num_classes.\n",
    "    # Then cross-entropy this vector with the actual label value. It should be noted that this function returns a vector.\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=t_logits+1e-10, labels=tf.reshape(labels,[-1])) \n",
    "    cem = tf.reduce_mean(ce) # Then average the obtaine d vector to get loss\n",
    "    loss_re =tf.get_collection('losses')\n",
    "    \n",
    "    loss = cem + tf.add_n(tf.get_collection('losses')) # Add regularization losses\n",
    "    \n",
    "    # Realize an exponential reduction of the learning rate, which can make the model quickly approach the better solution in the early stages of training, and ensure that the model does not fluctuate too much in the later stages of training\n",
    "    # Calculation formula: Decayed_learning_rate = learining_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        train_num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)  # When staircase = True, (global_step / decay_steps) is converted to an integer to select different attenuation methods\n",
    "\n",
    "    # # Pass in the learning rate, construct an optimizer that implements the gradient descent algorithm, and reduce the loss by using minimize to update the list of variables to be trained\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # To implement a moving average model, the parameter MOVING_AVERAGE_DECAY is used to control the speed of model updates. During the training process, a shadow variable is maintained for each variable, and the initial value of this shadow variable is maintained.\n",
    "    # Is the initial value of the corresponding variable, each time the variable is updated, the shadow variable is updated accordingly\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())  \n",
    "    Accuracy_value_,nums,label_,logit_= model.accuracy(t_logits,tf.cast(labels,tf.int32))\n",
    "    tf.summary.scalar('accuracy_value',Accuracy_value_)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.control_dependencies([train_step, ema_op]): # 将train_step 和ema_op 两个训练操作绑定到 train_op 上\n",
    "        train_op = tf.no_op(name='train')\n",
    "    saver = tf.train.Saver() # Instantiate a saver that saves and restores variables\n",
    "  \n",
    "    Accuracy_value = []\n",
    "    Loss_value = []\n",
    "    true_label = []\n",
    "    pred_label = []   \n",
    "    sess = tf.Session() # Create a session and manage it via a context manager in Python\n",
    "    train_writer = tf.summary.FileWriter('model_folder/logs', sess.graph)\n",
    "    init_op = tf.global_variables_initializer() # Initialize variables in calculation graphs\n",
    "    sess.run(init_op)\n",
    "    classes = [\"BENIGN\",\"ATTACK\"]\n",
    "    \n",
    "    \n",
    "    #     for Epo in range(0,EPOCHES):\n",
    "    #         print('epoch为:',Epo)\n",
    "    #         Accuracy_value_Epo = []\n",
    "    for index in range(0,Iteration):   \n",
    "        feature,label = next_batch_train(index,BATCH_SIZE)\n",
    "        \n",
    "        # _, loss_value_, accuracy_value,nums,reshape_logit_x,lable_x=session.run([t_optimizer, loss_value,Accuracy_value_,num,reshape_logits,lables], feed_dict={features:feature,labels:label,keep_prob:0.5})\n",
    "        #_,loss_value_, accuracy_value,nums_,label_s,logit_s=session.run([t_optimizer,loss_value,Accuracy_value_,nums,label_,logit_],feed_dict={features:feature,labels:label,keep_prob:0.5})\n",
    "        _, loss_value_, step,accuracy_value,nums_,label_s,logit_s = sess.run([train_op, loss, global_step,Accuracy_value_,nums,label_,logit_], feed_dict={features:feature,labels:label,keep_prob:0.5})\n",
    "        # t_logit = session.run([t_logits],feed_dict={features:feature,labels:label,keep_prob:0.5})\n",
    "        Accuracy_value.append(accuracy_value)\n",
    "        #  Accuracy_value_Epo.append(accuracy_value)\n",
    "        Loss_value.append(loss_value_)\n",
    "        true_label.extend(label_s)\n",
    "        pred_label.extend(logit_s)\n",
    "       \n",
    "        #batch_xs,batch_ys = mnist.train.next_batch(100)\n",
    "        if index % 1000 == 0:\n",
    "            print('index:', index, ' loss_value:', loss_value_, ' accuracy_value:', accuracy_value)\n",
    "            # train_writer.add_summary(loss_value, index)\n",
    "            #saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME), global_step=global_step)\n",
    "            saver.save(sess, os.path.join('model_folder/saver', 'model.ckpt'))\n",
    "    #sess.close()\n",
    "    #sess = tf.InteractiveSession()\n",
    "    op=tf.confusion_matrix(true_label,pred_label)\n",
    "    confusion_matrix = sess.run(op)\n",
    "    acc = model.accuracyofeachclass(confusion_matrix, true_label, num_classes = 2)\n",
    "    print(\"Accuracy of two classes：\",acc)\n",
    "    Epoch_accracy=model.Epoch_accuracy_mean(Accuracy_value)\n",
    "    # Calculate the accuracy of each category \n",
    "    # loss value\n",
    "    #get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "    #precision,accBA=Test_model.evaluation()     \n",
    "    train_writer.close()\n",
    "    #coord.request_stop()\n",
    "    #coord.join(threads)\n",
    "    return Accuracy_value,acc,Loss_value,Epoch_accracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
