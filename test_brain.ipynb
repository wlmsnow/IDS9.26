{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Configuration.ipynb\n",
      "importing Jupyter notebook from data_Generator.ipynb\n",
      "importing Jupyter notebook from runs.ipynb\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from Utils import NotebookFinder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Configuration import printPeriodic,setDbgPrint,null\n",
    "from data_Generator import dataGenerator\n",
    "\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runTest():\n",
    "    def __init__(self,**kwargs):\n",
    "        self.actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_features = 76\n",
    "        self.built_net()\n",
    "        self.TPcount = 0\n",
    "        self.FPcount = 0\n",
    "        self.TNcount =0\n",
    "        self.FNcount = 0\n",
    "        # sess = tf.InteractiveSession()\n",
    "        # self.saver.restore(sess, 'my_net/my_test_model.ckpt-20000')  # Load parameter\n",
    "        self.params = []\n",
    "        self.cost_his = []\n",
    "        self.time = 0\n",
    "        self.flag = 0\n",
    "        self.Kcount = kwargs.get('Kcount')\n",
    "        self.list = dataGenerator(self.flag,self.Kcount)\n",
    "        # self.label = data_Generator_label(self.flag)\n",
    "        self.reward_history = []\n",
    "        self.count_history =[]\n",
    "        self.Pr_history = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "        self.Rc_history = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "        self.F1_history = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
    "        self.count=np.zeros((15, 4))\n",
    "        self.time_env_state = self.time_env_state ={\"next\":{},\"current\":{}}\n",
    "        target_net = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        saver1 = tf.train.Saver(target_net)\n",
    "        self.sess = tf.Session()\n",
    "        #Upload model parameters\n",
    "       # model_file=tf.train.latest_checkpoint('my_net/')\n",
    "        #saver1.restore(self.sess,model_file)\n",
    "        saver1.restore(self.sess, \"./my_net/target_net.ckpt-370000\")\n",
    "    def built_net(self):\n",
    "        tf.reset_default_graph()\n",
    "        # Reconstruction neural network model.Only one of the neural networks is kept here.\n",
    "        # c_names, n_l1, w_initializer, b_initializer = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
    "        n_l1 = 10\n",
    "        self.env_state_ = tf.placeholder(tf.float32, [None, self.n_features], name='env_state_')\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            with tf.variable_scope(\n",
    "                    'l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], collections=c_names)\n",
    "                l1 = tf.nn.tanh(tf.matmul(self.env_state_, w1) + b1)\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "\n",
    "    def choose_action(self, env_state):\n",
    "        env_state = env_state[np.newaxis, :]\n",
    "        actions_value = self.sess.run(self.q_next,feed_dict={self.env_state_: env_state})\n",
    "        action = np.argmax(actions_value)\n",
    "        return action\n",
    "\n",
    "    def data(self):\n",
    "        Network_data = self.list[self.time]\n",
    "        return Network_data\n",
    "\n",
    "    def update_State(self):\n",
    "        self.time_env_state[\"next\"][\"feature\"] = self.data()[0:76]\n",
    "        self.time_env_state[\"next\"][\"label\"] = self.data()[76]\n",
    "        self.time += 1\n",
    "        return self.time_env_state[\"next\"]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_env_state[\"next\"] = self.update_State()\n",
    "        return self.time_env_state[\"next\"]\n",
    "    def reward(self, action,state):\n",
    "\n",
    "        self.time_env_state[\"current\"] = state\n",
    "        #print(self.time_env_state[\"current\"] )\n",
    "        label = self.time_env_state[\"current\"][\"label\"]\n",
    "        if action == self.time_env_state[\"current\"][\"label\"]:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "\n",
    "        return reward,label\n",
    "\n",
    "    def step(self, action,state):\n",
    "\n",
    "        value = 0\n",
    "        min_value = np.Inf\n",
    "        action_key = \"\"\n",
    "\n",
    "        #self.log('{}: action {} has min. value {}\\n'.format(self.time, action_key, min_value), period=self.statusPeriod,\n",
    "        #         counter=self.time)\n",
    "        # next_state = action\n",
    "        reward,label = self.reward(action,state)\n",
    "        self.reward_ = reward\n",
    "\n",
    "        self.count_history.append(self.reward_)\n",
    "        # self.correct_rate.append(self.count/(len(self.count_history)))\n",
    "\n",
    "        self.state = self.update_State()\n",
    "\n",
    "        return  self.time_env_state[\"next\"], reward, label\n",
    "\n",
    "    def run_(self,cfg=None,):\n",
    "        numEpisodes = cfg['numEpisodesTest']\n",
    "        maxSteps = cfg['maxStepsTest']\n",
    "        dbgPrint = cfg.get('dbgPrint', null)\n",
    "        if self.Kcount == 0:\n",
    "            maxSteps = 943583\n",
    "        elif self.Kcount == 1:\n",
    "            maxSteps = 943579\n",
    "        else:\n",
    "            maxSteps = 943574\n",
    "        statusPeriod = cfg.get('statusPeriod', 1)\n",
    "        for episode in range(1,numEpisodes+1):\n",
    "            # initial observation\n",
    "            step = 0\n",
    "            env_state_1 = self.reset()\n",
    "            while step < maxSteps:\n",
    "                #print('{}: current env = {}\\n'.format(self.time, self.time_env_state))\n",
    "                env_state = env_state_1[\"feature\"]\n",
    "                env_state = np.hstack(env_state)\n",
    "                action = self.choose_action(env_state)\n",
    "                # print('{}: action_ = {}, observation = {}\\n'.format(self.time, action_, observation))\n",
    "                observation_,reward,label = self.step(action,env_state_1)\n",
    "\n",
    "                label = int(label)\n",
    "                action = int(action)\n",
    "                \n",
    "                counti =0\n",
    "                for i in [label,action]:\n",
    "                   # if i==1:\n",
    "                        # print(\"label= {},action = {}\".format(label,action))\n",
    "                        #print(\"i = {}\".format(i))\n",
    "                    \n",
    "                    if label == i and action == i:  # TP\n",
    "                        self.count[i][0]+= 1\n",
    "                        #print(\"{}kkkkkkkkkkkkkkkkkkkkkkkk\".format(i))\n",
    "                        if counti == 1:\n",
    "                            self.count[i][0] -=1\n",
    "                        counti += 1\n",
    "\n",
    "                        # print('kkkkkk',self.count[i][0])\n",
    "                    elif action == i and label != i:  # FP\n",
    "                        self.count[i][1] += 1\n",
    "                        # print('kkkkkk',self.count[i][1])\n",
    "\n",
    "                    elif action != i and label != i:  # TN\n",
    "                        self.count[i][2] += 1\n",
    "                        #print('kkkkkk',self.count[i][2])\n",
    "                    elif action != i and label == i:  # FN\n",
    "                        self.count[i][3] += 1\n",
    "                       # print('kkkkkk',self.count[i][3])\n",
    "                   #print(self.count[i][0] + self.count[i][3],self.count[i][0] + self.count[i][1])\n",
    "                    \n",
    "                    if ((self.count[i][0] + self.count[i][3]) != 0 and (self.count[i][0] + self.count[i][1]) != 0):\n",
    "                        Pr = self.count[i][0] / (self.count[i][0] + self.count[i][1])\n",
    "                        Rc = self.count[i][0] / (self.count[i][0] + self.count[i][3])\n",
    "                        F1 = 2 / (1 / Pr + 1 / Rc)\n",
    "                        self.Rc_history[i].append(self.count[i][0]/(self.count[i][0]+self.count[i][3]))\n",
    "                        self.Pr_history[i].append(self.count[i][0]/(self.count[i][0]+self.count[i][1]))\n",
    "                        #print('{}___Rc = {},Pr = {},F1 = {}'.format(i,Rc,Pr,F1))\n",
    "                        self.F1_history[i].append(F1)\n",
    "                        \n",
    "\n",
    "\n",
    "                env_state_1 =  observation_\n",
    "                step += 1\n",
    "        plt.figure()\n",
    "        for i in range(1,16):\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot(self.Pr_history[i-1], '.-')\n",
    "            plt.xlabel('Test Step')\n",
    "            plt.ylabel('Pr_{}'.format(i-1))\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(self.Rc_history[i-1], '.-')\n",
    "            plt.xlabel('Test Step')\n",
    "            plt.ylabel('Rc_{}'.format(i-1))\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(\n",
    "                self.F1_history[i-1], '.-')\n",
    "            plt.xlabel('Test Step')\n",
    "            plt.ylabel('F1')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        plt.figure()\n",
    "        plt.plot(\n",
    "            np.divide( np.cumsum([1 if i >= 10 else 0 for i in self.count_history]), np.arange(len(self.count_history)) + 1),\n",
    "            '.-')\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Correct rate')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
