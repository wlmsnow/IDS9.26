{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Set hyper parameters of neural network\n",
    "# Define the feature size and number of channels that the neural network can receive\n",
    "FEATURES_SIZE = 9\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "# Define the size and number of first-level convolution kernels\n",
    "CONV1_SIZE = 5\n",
    "CONV1_KERNEL_NUM = 32\n",
    "\n",
    "# Define the size and number of second-level convolution kernels\n",
    "CONV2_SIZE = 5\n",
    "CONV2_KERNEL_NUM = 64\n",
    "\n",
    "# Define the number of neurons in the third fully connected layer\n",
    "FC_SIZE = 512\n",
    "\n",
    "# Define the number of neurons in the fourth fully connected layer\n",
    "OUTPUT_NODE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(features,regularizer,keep_prob):\n",
    "    \"\"\"Build the CIFAR-10 model\n",
    "    conv1-->pooling1-->conv2-->pooling_2-->local3-->local4\n",
    "    @param images: feed data[batch_size, height, width, 1][64,9,9,3]\n",
    "    @return: logits: Prediction vector\n",
    "    \"\"\"\n",
    "    x_features = tf.reshape(features, [-1, FEATURES_SIZE,FEATURES_SIZE, 1])\n",
    "\n",
    "    ## convl layer ##\n",
    "    W_conv1 = weight_variable([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_KERNEL_NUM],regularizer) # kernel 5*5, channel is 1, out size 32，regularizer is the regularization parameter。\n",
    "    b_conv1 = bias_variable([CONV1_KERNEL_NUM])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_features,W_conv1) + b_conv1)  # output size 9*9*32\n",
    "    h_pool1 = max_pool_2x2(h_conv1)                          \n",
    "\n",
    "    ## conv2 layer ##\n",
    "    W_conv2 =weight_variable([CONV2_SIZE,CONV2_SIZE,CONV1_KERNEL_NUM, CONV2_KERNEL_NUM],regularizer) # kernel 5*5, in size 32, out size 64\n",
    "    b_conv2 =bias_variable([CONV2_KERNEL_NUM])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  # output size 9*9*64\n",
    "    h_pool2 = max_pool_2x2(h_conv2)                          \n",
    "\n",
    "    \n",
    "    pool_shape = h_pool2.get_shape().as_list()\n",
    "    \n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    # reshaped = tf.reshape(h_pool2, [pool_shape[0], nodes])\n",
    "    reshaped = tf.layers.flatten(h_pool2)\n",
    "    # Initialize the weight of the fully connected layer and add regularization\n",
    "    fc1_w = weight_variable([nodes, FC_SIZE], regularizer)\n",
    "    # Initialize the bias item of the fully connected layer\n",
    "    fc1_b = bias_variable([FC_SIZE])\n",
    "    \n",
    "\n",
    "    # Do matrix multiplication of the converted reshaped vector and weight fc1_w, then add the offset, and finally use relu to activate\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_w) + fc1_b)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    # If it is in the training phase, use dropout on the output of this layer, that is, randomly disable half of the neurons in the output of this layer. It is set to avoid overfitting. It is generally only used in the fully connected layer.\n",
    "    # Implement the forward propagation process of the fourth fully connected layer,and initialize the variables corresponding to the fully connected layer.\n",
    "    \n",
    "    fc2_w = weight_variable([FC_SIZE, OUTPUT_NODE], regularizer)\n",
    "    fc2_b = bias_variable([OUTPUT_NODE])\n",
    "    t_logits = tf.matmul(fc1, fc2_w) + fc2_b\n",
    "    return t_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs:v_xs, keep_prob:1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1),tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs:v_xs,ys:v_ys,keep_prob:1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,regularizer):\n",
    "    initial = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "    if regularizer != None: \n",
    "        tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(initial)) \n",
    "    return initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "   # initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return  b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    # stride[1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] =1\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")  # padding=\"SAME\"用零填充边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,1,1,1], strides=[1,1,1,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    reshape_logits = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    accuracy_value = tf.reduce_mean(tf.cast(tf.equal(reshape_logits, labels), dtype=tf.float32))\n",
    "    num=tf.cast(tf.equal(reshape_logits, labels), dtype=tf.float32)\n",
    "    return accuracy_value,num,labels,reshape_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Epoch_accuracy_mean(accuracy):\n",
    "    Epoch_accuracy = np.mean(accuracy)\n",
    "    return Epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nums(true_labels, num_classes):\n",
    "    initial_value = 0\n",
    "    list_length = num_classes\n",
    "    list_data = [ initial_value for i in range(list_length)]\n",
    "    for i in range(0, num_classes):\n",
    "        list_data[i] = true_labels.count(i)\n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyofeachclass(confusion_matrix, true_labels, num_classes):\n",
    "    # Number of test samples in each category\n",
    "    list_data = count_nums(true_labels, num_classes)\n",
    "    # Number of correct classifications in each category\n",
    "    initial_value = 0\n",
    "    list_length = num_classes\n",
    "    true_pred = [ initial_value for i in range(list_length)]\n",
    "    for i in range(0,num_classes):\n",
    "        true_pred[i] = confusion_matrix[i][i]\n",
    " \n",
    "    # Calculate the correct rate of each sample being correctly classified\n",
    "    acc = []\n",
    "    for i in range(0, num_classes):\n",
    "        acc.append(0)\n",
    " \n",
    "    for i in range(0,num_classes):\n",
    "        acc[i] = true_pred[i] / list_data[i]\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
